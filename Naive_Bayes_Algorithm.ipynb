{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRuao2g3694A"
      },
      "source": [
        "# Welcome to this tutorial on the Naive Bayes Algorithm! \n",
        "\n",
        "By: Spencer Karp (VP of Analytics) spk61@georgetown.edu \\\\\n",
        "Fall 2022 \\\\\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWITjmke8ASp"
      },
      "source": [
        "In this Colab we will build the Naive Bayes Algorithm from scratch with explanations along the way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbVFjfPbd_te"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTfF8r3SeE6M"
      },
      "source": [
        "Here's a tutorial on how to build the model from scratch. We will not use any tools from the sklearn library so we can understand the theory behind the model before we implement it in practice. We will only use tools from the pandas and math libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm0UUQsSA7l0"
      },
      "source": [
        "### Getting the data\n",
        "Let's start by importing our data. The data we will be using gives us the blood glucose levels as well as the blood pressures of almost 1000 patients. Of this group, some of the patients are diabetic and some are not. Let's see if we can build an algorithm to predict patients with diabetes by examining just their glucose levels and blood pressure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "OsbY46Ec-oLq"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoXCckFd6cxF",
        "outputId": "4147d223-fdb5-435a-fc5a-76da04b0fe37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   glucose  bloodpressure  diabetes\n",
            "0       40             85         0\n",
            "1       40             92         0\n",
            "2       45             63         1\n",
            "3       45             80         0\n",
            "4       40             73         1\n"
          ]
        }
      ],
      "source": [
        "# import data\n",
        "data = pd.read_csv('Naive-Bayes-Classification-Data.csv')\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJYRBex1BxAv"
      },
      "source": [
        "Since the data is already randomized, we can just split the data in two. We'll use 30% of the data to train the model and the other 70% to test it. It should be noted that we will not be using a validation data set. A validation set can be used to prevent overfitting. The data we split now will be used to test our functions as we build the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHC7ZO4sALZ0",
        "outputId": "adc1d870-8a4f-4097-e508-bb1942d4ca5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     glucose  bloodpressure  diabetes\n",
            "0         40             85         0\n",
            "1         40             92         0\n",
            "2         45             63         1\n",
            "3         45             80         0\n",
            "4         40             73         1\n",
            "..       ...            ...       ...\n",
            "295       50             65         1\n",
            "296       60             77         1\n",
            "297       40             68         1\n",
            "298       45             88         1\n",
            "299       50             77         1\n",
            "\n",
            "[300 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "trainingData = data[:300]\n",
        "testingData = data[300:]\n",
        "print(trainingData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGe0ZivKuo3s"
      },
      "source": [
        "### Building the training components\n",
        "Let's start by building a function to organize the training data. We'll take the last column of the dataset (which holds our classifications) and build a dictionary with keys that are named with the different classifications. We'll use this organized dictionary to get our probabilities for the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0IS7YZqfHfYW"
      },
      "outputs": [],
      "source": [
        "def organize_data(data):\n",
        "  # Find all possible classifications\n",
        "  unique = data.iloc[:,-1].unique()\n",
        "  organizedData = {}\n",
        "\n",
        "  # Build dictionary by organizing data for each classification\n",
        "  for i in unique:\n",
        "    temp = data.loc[data.iloc[:,-1] == i]\n",
        "    organizedData[i] = temp\n",
        "\n",
        "  return organizedData"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test this on our training set\n",
        "organizedTrainingData = organize_data(trainingData)\n",
        "\n",
        "for key in organizedTrainingData:\n",
        "  print(key)\n",
        "  print(organizedTrainingData[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeEEw4r8teGy",
        "outputId": "5d9f5b3d-11ec-4c6d-82e4-4b8944904100"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "     glucose  bloodpressure  diabetes\n",
            "0         40             85         0\n",
            "1         40             92         0\n",
            "3         45             80         0\n",
            "5         45             82         0\n",
            "6         40             85         0\n",
            "..       ...            ...       ...\n",
            "277       40             82         0\n",
            "279       45             90         0\n",
            "282       50             83         0\n",
            "290       45             92         0\n",
            "293       40             87         0\n",
            "\n",
            "[152 rows x 3 columns]\n",
            "1\n",
            "     glucose  bloodpressure  diabetes\n",
            "2         45             63         1\n",
            "4         40             73         1\n",
            "7         30             63         1\n",
            "8         65             65         1\n",
            "10        35             73         1\n",
            "..       ...            ...       ...\n",
            "295       50             65         1\n",
            "296       60             77         1\n",
            "297       40             68         1\n",
            "298       45             88         1\n",
            "299       50             77         1\n",
            "\n",
            "[148 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdabWEoGui1G"
      },
      "source": [
        "Now we are going to summarize our data set. For each classification, we will find the mean and standard deviation of each feature. We will also take the counts of each to aid in our probability calculations.\n",
        "\n",
        "We'll return a dictionary with keys that are the classifications and within each key, a summary for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "C4QOqi2zPLmc"
      },
      "outputs": [],
      "source": [
        "def summarize_by_class(data):\n",
        "\n",
        "  summaries = dict()\n",
        "\n",
        "  # Build a summary for each classification (classType)\n",
        "  for classType in data:\n",
        "    typeSummary=[]\n",
        "\n",
        "    # Cycle through each feature and take summaries\n",
        "    for column in data[classType]:\n",
        "      summary = [data[classType][column].values.mean(), data[classType][column].values.std(), len(data[classType][column].values)]\n",
        "      typeSummary.append(summary)\n",
        "\n",
        "    # Remove summary of classification column\n",
        "    del(typeSummary[-1])\n",
        "    \n",
        "    # Add summaries for the classification\n",
        "    summaries[classType] = typeSummary\n",
        "\n",
        "  return summaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test it...\n",
        "summaries = summarize_by_class(organizedTrainingData)\n",
        "\n",
        "for key in summaries:\n",
        "  print(key)\n",
        "  print(summaries[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bXQNwaIt-0B",
        "outputId": "6601433b-3ac0-49a9-df2d-0b204443ed84"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[[43.58552631578947, 3.1632014074422012, 152], [87.03947368421052, 4.697257849207244, 152]]\n",
            "1\n",
            "[[45.0, 9.263412481953441, 148], [71.01351351351352, 5.667577878298113, 148]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hdh9Aaxaw9Z"
      },
      "source": [
        "### Building out testing components\n",
        "\n",
        "Let's build our inverst gaussian distribution function based on the equation we have above. As a reminder, the formula looks like this:\n",
        "\n",
        "$f(x|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "dANgSfInjhck"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "from math import pi\n",
        "from math import exp\n",
        "\n",
        "def calculate_probability(x, mean, stdev):\n",
        "\texponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
        "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCg00SrJk91C"
      },
      "source": [
        "Next, we're going to write a function that takes in a row from the testing dataset and uses Bayes's Theorem to calculate the probability that the items belong to each classification. We'll begin by creating an empty dictionary. Then for each possible classification, we'll calculate the probability that the item belongs there. We do this using the expanded form of bayes theorem that is shown above. We'll call the calculate_probability function each time and multiply these together to get each probability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0Fd6De83mOUQ"
      },
      "outputs": [],
      "source": [
        "def calculate_class_probabilities(summaries, row):\n",
        "  # Sum up the total about of rows in training data - this will be used in the P(type) probability\n",
        "  total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "  probabilities = dict()\n",
        "\n",
        "  # Cycle through each classification possible\n",
        "  for classType, class_summaries in summaries.items():\n",
        "\n",
        "    # Calculates our p(type) term\n",
        "    probabilities[classType] = summaries[classType][0][2]/float(total_rows)\n",
        "\n",
        "    # For each feature, we'll calculate p(type | feature) and multiply these to get the total probability\n",
        "    for i in range(len(class_summaries)):\n",
        "      mean, stdev, count = class_summaries[i]\n",
        "      probabilities[classType] *= calculate_probability(row[i], mean, stdev)\n",
        "      \n",
        "  # We return a dictionary containing the probabilities of each classification\n",
        "  return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets test it\n",
        "print(testingData.iloc[260,:])\n",
        "\n",
        "print(calculate_class_probabilities(summaries, testingData.iloc[260,:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDRsjVUXubgc",
        "outputId": "4159ef28-ac19-45de-e6fd-d787a71445d9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glucose          45\n",
            "bloodpressure    73\n",
            "diabetes          1\n",
            "Name: 560, dtype: int64\n",
            "{0: 5.6403433888965184e-05, 1: 0.001406421472597329}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk6ONfHlZ62I"
      },
      "source": [
        "Notice how the probability for class 1 is larger than that of class 0. Looks like our model correctly identified this row to be someone with Diabetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRGxcj3OaGox"
      },
      "source": [
        "In the previous function, you'll probably have noticed that I had to examine the probabilities and tell you the model was working. This function will actually make a prediction from our given probabilities. It will first call the previous calculate_class_probabilities function and then based on that, it will identify the highest probability and slap the label on that row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "dAIKuSSI_PZp"
      },
      "outputs": [],
      "source": [
        "def predict(summaries, row):\n",
        "\tprobabilities = calculate_class_probabilities(summaries, row)\n",
        "\tbest_label, best_prob = None, -1\n",
        "\t\n",
        "\t# Cycle through each key of the probabilities dictionary and identify the highest one\n",
        "\tfor class_value, probability in probabilities.items():\n",
        "\t\tif best_label is None or probability > best_prob:\n",
        "\t\t\tbest_prob = probability\n",
        "\t\t\t# Pull the label too which will be our return value\n",
        "\t\t\tbest_label = class_value\n",
        "\treturn best_label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test it on that same row\n",
        "print(predict(summaries, testingData.iloc[260,:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fntlDCXRu0j9",
        "outputId": "f726c884-c9de-466f-faa7-ebf0624d558f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7u2wzngdItW"
      },
      "source": [
        "Now we can see this function actually made a prediction, and it's the right one! Pun intended ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x236LwZedhG5"
      },
      "source": [
        "### Running the Model\n",
        "\n",
        "Now, using everything we just built, let's run and test the model on the whole data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx9yquRUiCB0"
      },
      "source": [
        "Our first function will randomly split the data set into a training set and a testing set. The split will be based on the ratio we input (e.g. a .3 ratio will mean 30% of the data is used for training and the other 70% for testing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "x1zd4CZdU46x"
      },
      "outputs": [],
      "source": [
        "def train_test_split(data, ratio):\n",
        "  # Pull a random sample of the data set\n",
        "  train = data.sample(frac = ratio)\n",
        "  # Creating dataframe with rest of values\n",
        "  test = data.drop(train.index)\n",
        "\n",
        "  return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN_yKLp5ivP8"
      },
      "source": [
        "Now, we'll combine all of the functions we built into one naive_bayes function. We'll first organize the training data, then pull summaries of the training data. Finally, from this we'll make our predictions. We'll return an array of all the predictions for the testing set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "AXvp92DKTjLq"
      },
      "outputs": [],
      "source": [
        "def naive_bayes(trainingData, testingData):\n",
        "  # Organize training data and get summaries\n",
        "  organizedData = organize_data(trainingData)\n",
        "  summaries = summarize_by_class(organizedData)\n",
        "  \n",
        "  # Build out predictions\n",
        "  predictions = [];\n",
        "  for i in range(len(testingData)):\n",
        "    predictions.append(predict(summaries, testingData.iloc[i,:]))\n",
        "  \n",
        "  return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Yw468WkgfB"
      },
      "source": [
        "Next, we'll build a function to evaluate our algorithm (there are libraries that can do this, but we'll do this from scratch). It'll do two things simultaneously. It will give us an accuracy metric and build a confusion matrix. The accuracy metric is simple. We'll calculate the percentage of the testing data that was correctly identified. \\\\\n",
        "The confusion matrix is a bit more...confusing. The rows of the confusion matrix represent the actual classifications. The columns represent the model's classifications. The ultimate goal is to have all items on the main diagonal (0s match 0s, 1s match 1s etc.). An example can be seen a few cells below. \\\\\n",
        "Note: If your still confused about the confusion matrix, stay tuned. We'll be releasing a conceptual article about it on the HoyAlytics Medium soon... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bKbvqKG1ZkqC"
      },
      "outputs": [],
      "source": [
        "def evaluate_algorithm(testingSet, predictions):\n",
        "  # This variable will be used to calculate accuracy %\n",
        "  correct = 0;\n",
        "\n",
        "  # Use the amount of classes in the data to get the size of the confusion matrix\n",
        "  unique = testingSet.iloc[:,-1].unique() \n",
        "  rows, cols = len(unique), len(unique)\n",
        "  \n",
        "  # Build confusion matrix with 0s\n",
        "  confusionMatrix = [[0 for i in range(cols)] for j in range(rows)]\n",
        "\n",
        "  # Loop through all predictions\n",
        "  for i in range(len(predictions)):\n",
        "    # Pair the prediction with the actuall classification and place in appropriate column\n",
        "    confusionMatrix[testingSet.iloc[i,-1]][predictions[i]] += 1\n",
        "    \n",
        "    # If prediction and actual classification match, then prediction is correct\n",
        "    if predictions[i] == testingSet.iloc[i,-1]:\n",
        "      correct+=1\n",
        "  \n",
        "  return correct/len(predictions), confusionMatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WA5pwxNmiGH"
      },
      "source": [
        "Finally! Let's test our model. We'll first split the data (70% of our data will train the model), then send it into the model, and then evaluate it. Let's see what we get..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kGfr8C4VKb2",
        "outputId": "1641e972-ffd6-4665-91fc-2e4e8a8d457e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9354375896700143\n",
            "[319, 28]\n",
            "[17, 333]\n"
          ]
        }
      ],
      "source": [
        "trainingSet, testingSet = train_test_split(data, 0.3)\n",
        "\n",
        "predictions = naive_bayes(trainingSet, testingSet)\n",
        "accuracy, confusion = evaluate_algorithm(testingSet, predictions)\n",
        "\n",
        "print(accuracy)\n",
        "for row in confusion:\n",
        "  print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adyEsM1QmvWp"
      },
      "source": [
        "Looks like the model performed pretty well! Let's compare it to sklearn's naive bayes algorithm to ensure we're getting this right. We'll use the same train, test split and see what happens... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIBiOUPx1x_I",
        "outputId": "3bf7f8c9-3792-4775-845b-0cd776873818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9296987087517934\n",
            "[[327  25]\n",
            " [ 24 321]]\n"
          ]
        }
      ],
      "source": [
        "# Using sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "features = data.iloc[:,:-1]\n",
        "actual = data.iloc[:,-1]\n",
        "\n",
        "trainingFeatures, testingFeatures, trainingResults, testingResults = train_test_split(features, actual, test_size=0.7, random_state=8)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(trainingFeatures, trainingResults)\n",
        "skLearnPredictions = model.predict(testingFeatures)\n",
        "\n",
        "\n",
        "print(accuracy_score(testingResults, skLearnPredictions))\n",
        "print(confusion_matrix(testingResults, skLearnPredictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsOTZZssnahu"
      },
      "source": [
        "These results look very similar to ours. Ultimately, they should since we built the same model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn5rvEzdnvrr"
      },
      "source": [
        "And that's it. We've just built a Gaussian Naive Bayes predictor! If you're going to use this model on a new set of data it's important to remember a couple of things: \n",
        "\n",
        "1. All your features must be continuous. That's where the \"Gaussian\" comes in. \n",
        "2. This model assumes that all of the features are independent (hence the word \"naive\"). While that's very rare in the real world, we can still gain vaulable insights if our features are only lightly dependent on each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOkN2gqdrxxH"
      },
      "source": [
        "## End Notes\n",
        "\n",
        "I want to shout out Jason Brownlee and [his article](https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/) on Naive Bayes. Also check out further reading materials on Naive Bayes in my reading list on my Medium Page. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "15f4229950d6c897a2121f94c70adc085f189408b618920f442605fb64e6dd18"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}